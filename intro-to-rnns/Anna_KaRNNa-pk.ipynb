{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-03T08:43:22.655186Z",
     "start_time": "2017-12-03T08:43:12.754948Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-03T08:43:22.715417Z",
     "start_time": "2017-12-03T08:43:22.657224Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## converting chars to integers\n",
    "with open('anna.txt','r') as f:\n",
    "    text= f.read()\n",
    "\n",
    "vocab = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-03T08:43:22.727490Z",
     "start_time": "2017-12-03T08:43:22.717453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{')', 'K', 'T', '_', 'L', 'B', '`', 'o', 'X', 'l', 'P', '5', '(', ';', 'v', '8', 'n', 'C', 'h', 'e', '0', 'x', 'Y', ':', '3', '4', 'V', '\"', '*', 'F', 'j', '\\n', '!', 'R', 'Q', 'S', '/', '2', '%', '?', 'm', '.', 'y', 'f', 'q', 'G', 'Z', '6', 'i', 'D', 'u', 'I', 'z', 'p', 'r', '-', 'g', 's', 'b', 'W', '7', 'U', 'O', '9', '@', '&', 't', 'w', 'c', 'M', 'k', \"'\", ',', 'J', 'H', 'N', 'A', 'd', 'E', 'a', ' ', '$', '1'}\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(len(vocab))\n",
    "## we get all the unique terms used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-03T08:43:22.745498Z",
     "start_time": "2017-12-03T08:43:22.729457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{')': 0, 'K': 1, 'T': 2, '_': 3, 'L': 4, 'B': 5, '`': 6, 'o': 7, 'X': 8, 'l': 9, 'P': 10, '5': 11, '(': 12, ';': 13, 'v': 14, '8': 15, 'n': 16, 'C': 17, 'h': 18, 'e': 19, '0': 20, 'x': 21, 'Y': 22, ':': 23, '3': 24, '4': 25, 'V': 26, '\"': 27, '*': 28, 'F': 29, 'j': 30, '\\n': 31, '!': 32, 'R': 33, 'Q': 34, 'S': 35, '/': 36, '2': 37, '%': 38, '?': 39, 'm': 40, '.': 41, 'y': 42, 'f': 43, 'q': 44, 'G': 45, 'Z': 46, '6': 47, 'i': 48, 'D': 49, 'u': 50, 'I': 51, 'z': 52, 'p': 53, 'r': 54, '-': 55, 'g': 56, 's': 57, 'b': 58, 'W': 59, '7': 60, 'U': 61, 'O': 62, '9': 63, '@': 64, '&': 65, 't': 66, 'w': 67, 'c': 68, 'M': 69, 'k': 70, \"'\": 71, ',': 72, 'J': 73, 'H': 74, 'N': 75, 'A': 76, 'd': 77, 'E': 78, 'a': 79, ' ': 80, '$': 81, '1': 82}\n"
     ]
    }
   ],
   "source": [
    "v2i = {c:i for i,c in enumerate(vocab)}\n",
    "print(v2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-03T08:43:22.770127Z",
     "start_time": "2017-12-03T08:43:22.747504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ')', 1: 'K', 2: 'T', 3: '_', 4: 'L', 5: 'B', 6: '`', 7: 'o', 8: 'X', 9: 'l', 10: 'P', 11: '5', 12: '(', 13: ';', 14: 'v', 15: '8', 16: 'n', 17: 'C', 18: 'h', 19: 'e', 20: '0', 21: 'x', 22: 'Y', 23: ':', 24: '3', 25: '4', 26: 'V', 27: '\"', 28: '*', 29: 'F', 30: 'j', 31: '\\n', 32: '!', 33: 'R', 34: 'Q', 35: 'S', 36: '/', 37: '2', 38: '%', 39: '?', 40: 'm', 41: '.', 42: 'y', 43: 'f', 44: 'q', 45: 'G', 46: 'Z', 47: '6', 48: 'i', 49: 'D', 50: 'u', 51: 'I', 52: 'z', 53: 'p', 54: 'r', 55: '-', 56: 'g', 57: 's', 58: 'b', 59: 'W', 60: '7', 61: 'U', 62: 'O', 63: '9', 64: '@', 65: '&', 66: 't', 67: 'w', 68: 'c', 69: 'M', 70: 'k', 71: \"'\", 72: ',', 73: 'J', 74: 'H', 75: 'N', 76: 'A', 77: 'd', 78: 'E', 79: 'a', 80: ' ', 81: '$', 82: '1'}\n"
     ]
    }
   ],
   "source": [
    "i2v = dict(enumerate(vocab))\n",
    "print(i2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-03T08:49:07.852598Z",
     "start_time": "2017-12-03T08:49:07.597276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17 18 79 ..., 57 41 31]\n"
     ]
    }
   ],
   "source": [
    "## encoding the whole book\n",
    "encoded_text = np.array([v2i[c] for c in text], dtype=np.int32)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-03T08:49:07.870678Z",
     "start_time": "2017-12-03T08:49:07.854601Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    batch_size = n_seqs*n_steps #tot # of chars in each batch\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    ## round the batches\n",
    "    arr = arr[:batch_size*n_batches]\n",
    "    arr = arr.reshape(n_seqs,-1)\n",
    "    \n",
    "    for n in range(0, n_seqs, n_steps):\n",
    "        x = arr[:,n:n+n_steps]\n",
    "        y= np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-03T08:49:09.443997Z",
     "start_time": "2017-12-03T08:49:09.432934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.array([1,2,3,4,5,6])\n",
    "tmp.reshape(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-03T08:49:10.622167Z",
     "start_time": "2017-12-03T08:49:10.617159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = get_batches(encoded_text, 10, 50)\n",
    "type(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-03T08:49:11.399941Z",
     "start_time": "2017-12-03T08:49:11.394928Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-03T12:50:15.872038Z",
     "start_time": "2017-12-03T12:50:15.849451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17, 18, 79, 53, 66, 19, 54, 80, 82, 31],\n",
       "       [80, 79, 40, 80, 16,  7, 66, 80, 56,  7],\n",
       "       [14, 48, 16, 41, 31, 31, 27, 22, 19, 57],\n",
       "       [16, 80, 77, 50, 54, 48, 16, 56, 80, 18],\n",
       "       [80, 48, 66, 80, 48, 57, 72, 80, 57, 48],\n",
       "       [80, 51, 66, 80, 67, 79, 57, 31,  7, 16],\n",
       "       [18, 19, 16, 80, 68,  7, 40, 19, 80, 43],\n",
       "       [13, 80, 58, 50, 66, 80, 16,  7, 67, 80],\n",
       "       [66, 80, 48, 57, 16, 71, 66, 41, 80,  2],\n",
       "       [80, 57, 79, 48, 77, 80, 66,  7, 80, 18]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T12:14:01.182751Z",
     "start_time": "2017-11-28T12:14:01.175765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 3 47 32 31 15 23  9 20 46 76]\n",
      " [20 32 64 20 81 43 15 20 69 43]\n",
      " [29 18 81 16 76 76 33 30 23 57]\n",
      " [81 20 65 53  9 18 81 69 20 47]\n",
      " [20 18 15 20 18 57 21 20 57 18]\n",
      " [20 51 15 20 28 32 57 76 43 81]\n",
      " [47 23 81 20 70 43 64 23 20 48]\n",
      " [78 20 44 53 15 20 81 43 28 20]\n",
      " [15 20 18 57 81 66 15 16 20 74]\n",
      " [20 57 32 18 65 20 15 43 20 47]]\n",
      "\n",
      "y\n",
      " [[47 32 31 15 23  9 20 46 76 76]\n",
      " [32 64 20 81 43 15 20 69 43 18]\n",
      " [18 81 16 76 76 33 30 23 57 21]\n",
      " [20 65 53  9 18 81 69 20 47 18]\n",
      " [18 15 20 18 57 21 20 57 18  9]\n",
      " [51 15 20 28 32 57 76 43 81 80]\n",
      " [23 81 20 70 43 64 23 20 48 43]\n",
      " [20 44 53 15 20 81 43 28 20 57]\n",
      " [20 18 57 81 66 15 16 20 74 47]\n",
      " [57 32 18 65 20 15 43 20 47 23]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T17:47:38.944005Z",
     "start_time": "2017-11-28T17:47:38.936515Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(n_seqs, num_steps):\n",
    "    inputs = tf.placeholder(tf.int32, [n_seqs, num_steps], name='input')\n",
    "    target = tf.placeholder(tf.int32, [n_seqs, num_steps], name='target')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, target, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        x: Input tensor\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # That is, the shape should be batch_size*num_steps rows by lstm_size columns\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
